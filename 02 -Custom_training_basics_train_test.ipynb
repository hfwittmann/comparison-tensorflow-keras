{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02 -Custom_training_basics_train_test.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"njUadpnjIPnd","colab_type":"text"},"cell_type":"markdown","source":["# <font color=\"yellow\">Remarks</font>\n","This notebook is based on a Tutorial notebook on the tensorflow web site.\n","\n","1.\tCustom training: basics  |  TensorFlow. TensorFlow. https://www.tensorflow.org/tutorials/eager/custom_training. Published December 17, 2018. Accessed December 29, 2018.\n","\n","You can find some more insformation on eager execution, also on Tensorflow's website:\n","2.\tEager Execution  |  TensorFlow. TensorFlow. https://www.tensorflow.org/guide/eager. Published December 12, 2018. Accessed December 29, 2018.\n","\n","\n","\n","<font color=\"yellow\">We have marked sections that have differences with the original workbook in yellow</font>.\n","    \n","Author: H. Felix Wittmann\n","hfwittmann@gmail.com\n","\n","There is no association between the author and Tensorflow.\n"]},{"metadata":{"colab_type":"text","id":"5rmpybwysXGV"},"cell_type":"markdown","source":["##### Copyright 2018 The TensorFlow Authors."]},{"metadata":{"cellView":"form","colab_type":"code","id":"m8y3rGtQsYP2","colab":{}},"cell_type":"code","source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"hrXv0rU9sIma"},"cell_type":"markdown","source":["# Custom training: basics"]},{"metadata":{"colab_type":"text","id":"k2o3TTG4TFpt"},"cell_type":"markdown","source":["In the previous tutorial we covered the TensorFlow APIs for automatic differentiation, a basic building block for machine learning.\n","In this tutorial we will use the TensorFlow primitives introduced in the prior tutorials to do some simple machine learning.\n","\n","TensorFlow also includes a higher-level neural networks API (`tf.keras`) which provides useful abstractions to reduce boilerplate. We strongly recommend those higher level APIs for people working with neural networks. However, in this short tutorial we cover neural network training from first principles to establish a strong foundation."]},{"metadata":{"colab_type":"text","id":"3LXMVuV0VhDr"},"cell_type":"markdown","source":["## Setup"]},{"metadata":{"colab_type":"code","id":"PJ64L90aVir3","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"eMAWbDJFVmMk"},"cell_type":"markdown","source":["## Variables\n","\n","Tensors in TensorFlow are immutable stateless objects. Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!). To represent this state which needs to change over the course of your computation, you can choose to rely on the fact that Python is a stateful programming language:\n"]},{"metadata":{"colab_type":"code","id":"VkJwtLS_Jbn8","colab":{}},"cell_type":"code","source":["# Using python state\n","x = tf.zeros([10, 10])\n","x += 2  # This is equivalent to x = x + 2, which does not mutate the original\n","        # value of x\n","print(x)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"wfneTXy7JcUz"},"cell_type":"markdown","source":["TensorFlow, however, has stateful operations built in, and these are often more pleasant to use than low-level Python representations of your state. To represent weights in a model, for example, it's often convenient and efficient to use TensorFlow variables.\n","\n","A Variable is an object which stores a value and, when used in a TensorFlow computation, will implicitly read from this stored value. There are operations (`tf.assign_sub`, `tf.scatter_update`, etc) which manipulate the value stored in a TensorFlow variable."]},{"metadata":{"colab_type":"code","id":"itxmrMil6DQi","colab":{}},"cell_type":"code","source":["v = tf.Variable(1.0)\n","assert v.numpy() == 1.0\n","\n","# Re-assign the value\n","v.assign(3.0)\n","assert v.numpy() == 3.0\n","\n","# Use `v` in a TensorFlow operation like tf.square() and reassign\n","v.assign(tf.square(v))\n","assert v.numpy() == 9.0"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"-paSaeq1JzwC"},"cell_type":"markdown","source":["Computations using Variables are automatically traced when computing gradients. For Variables representing embeddings TensorFlow will do sparse updates by default, which are more computation and memory efficient.\n","\n","Using Variables is also a way to quickly let a reader of your code know that this piece of state is mutable."]},{"metadata":{"colab_type":"text","id":"BMiFcDzE7Qu3"},"cell_type":"markdown","source":["## Example: Fitting a linear model\n","\n","Let's now put the few concepts we have so far ---`Tensor`, `GradientTape`, `Variable` --- to build and train a simple model. This typically involves a few steps:\n","\n","1. Define the model.\n","2. Define a loss function.\n","3. Obtain training data.\n","4. Run through the training data and use an \"optimizer\" to adjust the variables to fit the data.\n","\n","In this tutorial, we'll walk through a trivial example of a simple linear model: `f(x) = x * W + b`, which has two variables - `W` and `b`. Furthermore, we'll synthesize data such that a well trained model would have `W = 3.0` and `b = 2.0`."]},{"metadata":{"colab_type":"text","id":"gFzH64Jn9PIm"},"cell_type":"markdown","source":["### Define the model\n","\n","Let's define a simple class to encapsulate the variables and the computation."]},{"metadata":{"colab_type":"code","id":"_WRu7Pze7wk8","colab":{}},"cell_type":"code","source":["class Model(object):\n","  def __init__(self):\n","    # Initialize variable to (5.0, 0.0)\n","    # In practice, these should be initialized to random values.\n","    self.W = tf.Variable(5.0)\n","    self.b = tf.Variable(0.0)\n","    \n","  def __call__(self, x):\n","    return self.W * x + self.b\n","  \n","model = Model()\n","\n","assert model(3.0).numpy() == 15.0"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"xa6j_yXa-j79"},"cell_type":"markdown","source":["### Define a loss function\n","\n","A loss function measures how well the output of a model for a given input matches the desired output. Let's use the standard L2 loss."]},{"metadata":{"colab_type":"code","id":"Y0ysUFGY924U","colab":{}},"cell_type":"code","source":["def loss(predicted_y, desired_y):\n","  return tf.reduce_mean(tf.square(predicted_y - desired_y))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"qutT_fkl_CBc"},"cell_type":"markdown","source":["### <font color='yellow'>Obtain training data</font>\n","\n","Let's synthesize the training data with some noise."]},{"metadata":{"colab_type":"code","id":"gxPTb-kt_N5m","colab":{}},"cell_type":"code","source":["import numpy as np\n","nOfDatapoints = 1000\n","W_TRUE, b_TRUE = 3.0, 2.0\n","np.random.seed(314)\n","inputs = np.random.normal(size=nOfDatapoints)\n","noise = np.random.normal(size=nOfDatapoints)\n","outputs = W_TRUE * inputs + b_TRUE + noise"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8yZvLSXFIPn3","colab_type":"text"},"cell_type":"markdown","source":["### <font color='yellow'>Split into train/test</font>"]},{"metadata":{"id":"4a20SkSHIPn3","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","inputs_train, inputs_test, outputs_train, outputs_test = \\\n","    train_test_split(inputs, outputs, test_size=0.33, random_state=42)\n","# outputs = outputs.reshape(-1,1)  # reshape to match keras output"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"_eb83LtrB4nt","scrolled":true,"colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.scatter(inputs,outputs,c='b', alpha=0.5, label='Data')\n","plt.scatter(inputs_train, outputs_train, c='y', alpha=0.2, label='Train')\n","plt.scatter(inputs_test, outputs_test, c='g', alpha=0.2, label='Test')\n","plt.legend()## Explore data und Model\n","\n","model = Model()\n","\n","print('Current loss: '),\n","print(loss(model(inputs), outputs).numpy())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mjZA8UHIIPn7","colab_type":"text"},"cell_type":"markdown","source":["### <font color='yellow'>Explore data und Model before training</font>"]},{"metadata":{"colab_type":"text","id":"-50nq-wPBsAW"},"cell_type":"markdown","source":["Before we train the model let's visualize where the model stands right now. We'll plot the model's predictions in black and the testting data in blue."]},{"metadata":{"id":"F8W-Fc4YIPn8","colab_type":"code","colab":{}},"cell_type":"code","source":["model = Model()\n","\n","prediction_test = model(inputs_test).numpy()\n","plt.scatter(inputs_test, outputs_test, c = 'b', alpha=0.5, label = 'Test Data')\n","plt.scatter(inputs_test, prediction_test, c = 'k', alpha=0.5, label = 'Initial Model Guess')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"sSDP-yeq_4jE"},"cell_type":"markdown","source":["### Define a training loop\n","\n","We now have our network and our training data. Let's train it, i.e., use the training data to update the model's variables (`W` and `b`) so that the loss goes down using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer` implementations. We'd highly recommend using those implementations, but in the spirit of building from first principles, in this particular example we will implement the basic math ourselves."]},{"metadata":{"id":"GkeVjuBRIPoA","colab_type":"text"},"cell_type":"markdown","source":["### <font color='yellow'> Define gradient tape </font>"]},{"metadata":{"colab_type":"code","id":"MBIACgdnA55X","colab":{}},"cell_type":"code","source":["def grad(model, inputs, outputs):\n","    with tf.GradientTape() as tape:\n","        current_loss = loss(model(inputs), outputs)\n","    return current_loss, tape.gradient(current_loss, [model.W, model.b] )"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"RwWPaJryD2aN"},"cell_type":"markdown","source":["Finally, let's repeatedly run through the training data and see how `W` and `b` evolve."]},{"metadata":{"colab_type":"code","id":"XdfkR223D9dW","colab":{}},"cell_type":"code","source":["learning_rate = 0.1\n","model = Model()\n","\n","# Collect the history of W-values and b-values to plot later\n","Ws, bs, losses = [], [], []\n","epochs = range(10)\n","for epoch in epochs:\n","    Ws.append(model.W.numpy())\n","    bs.append(model.b.numpy())\n","\n","    \n","    current_loss, [dW, db] = grad(model, inputs_train, outputs_train)\n","    \n","    losses.append(current_loss)\n","    \n","    model.W.assign_add(-learning_rate*dW)\n","    model.b.assign_add(-learning_rate*db)\n","\n","\n","    print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n","        (epoch, Ws[-1], bs[-1], current_loss))\n","\n","# Let's plot it all\n","plt.plot(epochs, Ws, 'r',\n","         epochs, bs, 'b')\n","plt.plot([W_TRUE] * len(epochs), 'r--',\n","         [b_TRUE] * len(epochs), 'b--')\n","plt.legend(['W', 'b', 'true W', 'true_b'])\n","plt.show()\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"hishP8UfIPoH","colab_type":"text"},"cell_type":"markdown","source":["### <font color='yellow'>A more abstract representation is the loss function</font>"]},{"metadata":{"scrolled":true,"id":"TVa7Af5kIPoI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Let's plot the history\n","plt.plot(losses)\n","plt.title('Evolution of loss')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g1sG0YDQIPoK","colab_type":"code","colab":{}},"cell_type":"code","source":["# Let's plot the history\n","plt.plot(losses)\n","plt.title('Evolution of loss')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"vPnIVuaSJwWz"},"cell_type":"markdown","source":["## Next Steps\n","\n","In this tutorial we covered `Variable`s and built and trained a simple linear model using the TensorFlow primitives discussed so far.\n","\n","In theory, this is pretty much all you need to use TensorFlow for your machine learning research.\n","In practice, particularly for neural networks, the higher level APIs like `tf.keras` will be much more convenient since it provides higher level building blocks (called \"layers\"), utilities to save and restore state, a suite of loss functions, a suite of optimization strategies etc.\n"]},{"metadata":{"id":"tmHAEi4cIPoM","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"lV7GMzavIPoO","colab_type":"text"},"cell_type":"markdown","source":["## <font color=\"yellow\">Explore data und Model after training</font>"]},{"metadata":{"id":"SB0Z4gLDIPoO","colab_type":"code","colab":{}},"cell_type":"code","source":["# model = Model()\n","\n","prediction_test = model(inputs_test).numpy()\n","plt.scatter(inputs_test, outputs_test, c = 'b', alpha=0.5, label = 'Test Data')\n","plt.scatter(inputs_test, prediction_test, c = 'k', alpha=0.5, label = 'Model Prediction')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"REjFcXFqIPoQ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"acibRAvNIPoS","colab_type":"text"},"cell_type":"markdown","source":["## <font color='yellow'>Explore loss function</font>"]},{"metadata":{"id":"FO0_4dW7IPoS","colab_type":"code","colab":{}},"cell_type":"code","source":["def myloss(W,b):\n","    return ((inputs_train * W + b - outputs_train) ** 2).mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fy_7fmqSIPoU","colab_type":"text"},"cell_type":"markdown","source":["##  <font color='yellow'>Calculate loss surface</font>"]},{"metadata":{"id":"8jERvLDhJAZO","colab_type":"code","colab":{}},"cell_type":"code","source":["mean = [3,2]\n","cov = [[2,0],[0,2]]\n","Wb = np.random.multivariate_normal(mean=mean, cov=cov, size=10000)\n","W, b = Wb[:,0], Wb[:,1]\n","loss = [myloss(W,b) for (W,b) in Wb]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HLZFGfnuJC7A","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"apMbep8wJSZd","colab_type":"text"},"cell_type":"markdown","source":["## Prepare plotting"]},{"metadata":{"id":"N6mNHXnPJeV6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"oUfzS6JWJd6z","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"nWR6PjLVIPoX","colab_type":"code","colab":{}},"cell_type":"code","source":["# https://colab.research.google.com/notebooks/charts.ipynb#scrollTo=Xbey0KVpfIbQ\n","from plotly.offline import iplot, plot, init_notebook_mode\n","import plotly.graph_objs as go\n","\n","\n","def enable_plotly_in_cell():\n","  import IPython\n","  from plotly.offline import init_notebook_mode\n","  display(IPython.core.display.HTML('''\n","        <script src=\"/static/components/requirejs/require.js\"></script>\n","  '''))\n","  init_notebook_mode(connected=False)\n","\n","get_ipython().events.register('pre_run_cell', enable_plotly_in_cell)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hruqCaqbIPoZ","colab_type":"text"},"cell_type":"markdown","source":["### <font color='yellow'>Calculate the trace for the surface data</font>"]},{"metadata":{"id":"XVbX9-3rIPoa","colab_type":"code","colab":{}},"cell_type":"code","source":["trace = go.Scatter3d(\n","    x = W,\n","    y = b,\n","    z = loss,\n","    mode='markers',\n","    marker = dict(\n","        opacity = 0.5,\n","        size = 2,\n","        color = np.sin(loss)\n","    ),\n","    name = 'General Surface'\n",")\n","\n","data = [trace]\n","layout = go.Layout(\n","    title='Loss function',\n","    scene = dict (\n","        xaxis =  dict(title='W'),\n","        yaxis = dict(title='b'),\n","        zaxis = dict(title='loss')\n","    )\n",")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_8cr3orUIPoc","colab_type":"text"},"cell_type":"markdown","source":["### <font color='yellow'> Calculate the trace for the steps of the gradient based optimisation</font>"]},{"metadata":{"scrolled":false,"id":"eLzUhYaUIPoc","colab_type":"code","colab":{}},"cell_type":"code","source":["trace2 = go.Scatter3d(\n","    x = Ws,\n","    y = bs,\n","    z = np.array(losses),\n","    mode='lines+markers',\n","    marker = dict(\n","        opacity = 0.5,\n","        size = 5,\n","        color = 'black'\n","    ),\n","    name = 'Gradient-based Optimisation'\n",")\n","\n","data = [trace, trace2]\n","\n","layout = go.Layout(\n","    title='Loss function',\n","    scene = dict (\n","        xaxis =  dict(title='W'),\n","        yaxis = dict(title='b'),\n","        zaxis = dict(title='loss')\n","    )\n",")\n","\n","iplot({'data':data, 'layout': layout})\n","# cplot({'data':data, 'layout': layout},'loss function.html')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rFEeZqDEIPoe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"-UlI9nM_IPoh","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}